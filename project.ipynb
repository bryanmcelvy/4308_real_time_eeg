{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1UPn77Si3Ca"
      },
      "source": [
        "# Real-Time Seizure Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYM8qObI65ee"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldIjCgMFCe5Y"
      },
      "source": [
        "##### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssGj8h-oB6e2",
        "outputId": "413e589a-fd3a-4f35-af50-509129d57596"
      },
      "outputs": [],
      "source": [
        "''' Import libraries'''\n",
        "# Built-in Python libraries\n",
        "import os\n",
        "\n",
        "# Custom Functions\n",
        "from feature_extraction import *\n",
        "from log_reg_funcs import *\n",
        "\n",
        "# 3rd-Party Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal\n",
        "import seaborn as sns\n",
        "# from seaborn import heatmap\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q48PBfkwCxQC"
      },
      "source": [
        "##### Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dbPy32tZy6q",
        "outputId": "e5009252-5e53-4968-b1df-b989c5e75532"
      },
      "outputs": [],
      "source": [
        "'''Import dataset'''\n",
        "filename = 'datasets/dataset_all_filt.csv'\n",
        "dataset = pd.read_csv(filename) # Preprocessed EEG Dataset\n",
        "\n",
        "state = dataset['state'].to_numpy() # Correct labels for each time segment\n",
        "filename = dataset['filename'].to_list() # filename for each sample\n",
        "data = dataset.drop(columns=['state', 'filename']) # Voltage [uV]\n",
        "\n",
        "fs = 256 # Sampling Frequency [Hz]\n",
        "\n",
        "channels = data.keys().to_list()\n",
        "num_channels = len(channels)\n",
        "class_labels = {0:'Not Seizure', 1:'Seizure'} # classes to be predicted for each time segment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tp6mp8dDJLV"
      },
      "source": [
        "#### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "__S0vIxXB6e-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 22030, 9)\n"
          ]
        }
      ],
      "source": [
        "num_features = 9\n",
        "num_segments = int( len(data) / fs ) # number of 1-second time windows\n",
        "time = np.arange(num_segments, dtype=int)\n",
        "\n",
        "# Create np array with shape (num_segments, num_channels, num_features) \n",
        "x_data = np.zeros(\n",
        "    shape=(\n",
        "        num_channels,\n",
        "        num_segments,\n",
        "        num_features\n",
        "    ), dtype=np.float64)\n",
        "print(x_data.shape)\n",
        "\n",
        "# Perform feature extraction for each time segment across all channels\n",
        "for ch_num, ch in enumerate(channels):\n",
        "    for t in time:\n",
        "        start_idx = t * fs\n",
        "        end_idx = start_idx + fs\n",
        "        x_data[ch_num][t] = extractFeatures(data[ch][start_idx:end_idx])\n",
        "\n",
        "# Create y_data\n",
        "y_data = [0 if state[t*fs] == 0 else 1 for t in time]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Normalized Decay</th>\n",
              "      <th>Line Length</th>\n",
              "      <th>Mean Energy</th>\n",
              "      <th>Average Peak Amplitude</th>\n",
              "      <th>Average Valley Amplitude</th>\n",
              "      <th>Normalized Peak Amplitude</th>\n",
              "      <th>Peak Variation</th>\n",
              "      <th>Root Mean Square</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>22030.000000</td>\n",
              "      <td>22030.0</td>\n",
              "      <td>22030.000000</td>\n",
              "      <td>22030.000000</td>\n",
              "      <td>22030.000000</td>\n",
              "      <td>22030.000000</td>\n",
              "      <td>22030.000000</td>\n",
              "      <td>22030.0</td>\n",
              "      <td>22030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>43.671584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2511.263459</td>\n",
              "      <td>6967.685701</td>\n",
              "      <td>2.828552</td>\n",
              "      <td>2.820109</td>\n",
              "      <td>1.683976</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.020200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>50.313845</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3157.510281</td>\n",
              "      <td>20651.014009</td>\n",
              "      <td>0.788422</td>\n",
              "      <td>0.780746</td>\n",
              "      <td>1.241062</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.337018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>781.250000</td>\n",
              "      <td>382.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1395.000000</td>\n",
              "      <td>1121.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2974.000000</td>\n",
              "      <td>4127.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>678.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35827.000000</td>\n",
              "      <td>573564.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>757.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Area  Normalized Decay   Line Length    Mean Energy  \\\n",
              "count  22030.000000           22030.0  22030.000000   22030.000000   \n",
              "mean      43.671584               0.0   2511.263459    6967.685701   \n",
              "std       50.313845               0.0   3157.510281   20651.014009   \n",
              "min        0.000000               0.0      0.000000       0.000000   \n",
              "25%       15.000000               0.0    781.250000     382.000000   \n",
              "50%       26.000000               0.0   1395.000000    1121.000000   \n",
              "75%       50.000000               0.0   2974.000000    4127.000000   \n",
              "max      678.000000               0.0  35827.000000  573564.000000   \n",
              "\n",
              "       Average Peak Amplitude  Average Valley Amplitude  \\\n",
              "count            22030.000000              22030.000000   \n",
              "mean                 2.828552                  2.820109   \n",
              "std                  0.788422                  0.780746   \n",
              "min                  0.000000                  0.000000   \n",
              "25%                  2.000000                  2.000000   \n",
              "50%                  3.000000                  3.000000   \n",
              "75%                  3.000000                  3.000000   \n",
              "max                  5.000000                  5.000000   \n",
              "\n",
              "       Normalized Peak Amplitude  Peak Variation  Root Mean Square  \n",
              "count               22030.000000         22030.0      22030.000000  \n",
              "mean                    1.683976             0.0         55.020200  \n",
              "std                     1.241062             0.0         62.337018  \n",
              "min                     0.000000             0.0          0.000000  \n",
              "25%                     1.000000             0.0         19.000000  \n",
              "50%                     2.000000             0.0         33.000000  \n",
              "75%                     3.000000             0.0         64.000000  \n",
              "max                     6.000000             0.0        757.000000  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_df = pd.DataFrame(x_data[0], \n",
        "                    columns=[\"Area\", \"Normalized Decay\", \"Line Length\", \n",
        "                             \"Mean Energy\", \"Average Peak Amplitude\", \"Average Valley Amplitude\",\n",
        "                             \"Normalized Peak Amplitude\", \"Peak Variation\", \"Root Mean Square\"])\n",
        "x_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_remove = []\n",
        "for idx, feature in enumerate(x_df.columns):\n",
        "    if x_df[feature].mean() == 0 and x_df[feature].std() == 0: to_remove.append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Decay was removed.\n",
            "Peak Variation was removed.\n"
          ]
        }
      ],
      "source": [
        "to_remove = []\n",
        "for ch in x_data:\n",
        "    x_df = pd.DataFrame(ch, \n",
        "                    columns=[\"Area\", \"Normalized Decay\", \"Line Length\", \n",
        "                             \"Mean Energy\", \"Average Peak Amplitude\", \"Average Valley Amplitude\",\n",
        "                             \"Normalized Peak Amplitude\", \"Peak Variation\", \"Root Mean Square\"])\n",
        "    for idx, feature in enumerate(x_df.columns):\n",
        "        if x_df[feature].mean() == 0 and x_df[feature].std() == 0: \n",
        "            to_remove.append(idx)\n",
        "            \n",
        "to_remove_idx = list(set(to_remove))\n",
        "to_remove = x_df.columns[to_remove_idx].to_list()\n",
        "\n",
        "for feature in to_remove:\n",
        "    print(f\"{feature} was removed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MACto5tnqclq"
      },
      "source": [
        "#### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku1mGq5Ld7xp"
      },
      "outputs": [],
      "source": [
        "# Reshape into rank-2 array\n",
        "x_data = x_data.reshape((num_segments, num_channels*num_features))\n",
        "print(f\"New Shape = {x_data.shape}\")\n",
        "\n",
        "# Split into training/validation/test sets\n",
        "x_train, x_tv, y_train, y_tv = train_test_split(\n",
        "    x_data, y_data,\n",
        "    train_size=0.80, test_size=0.20,      # 50:50 split between training and test/validation sets\n",
        "    shuffle=True, random_state=42         # shuffles data using same random seed every time\n",
        ")\n",
        "\n",
        "x_test, x_val, y_test, y_val = train_test_split(\n",
        "    x_tv, y_tv,\n",
        "    train_size = 0.50, test_size = 0.50,   # Even split btw test and validation sets\n",
        "    shuffle=True, random_state=42         # same as above\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwdoA7v5vVH_"
      },
      "outputs": [],
      "source": [
        "# Store data in tensors\n",
        "x_train = tf.convert_to_tensor(x_train, dtype=np.float32)\n",
        "x_test = tf.convert_to_tensor(x_test, dtype=np.float32)\n",
        "x_val = tf.convert_to_tensor(x_val, dtype=np.float32)\n",
        "\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=np.float32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=np.float32)\n",
        "y_val = tf.convert_to_tensor(y_val, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmw-bVlFxY2R"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "x_norm_train = Normalizer(x_train)\n",
        "x_norm_test = Normalizer(x_test)\n",
        "x_norm_val = Normalizer(x_val)\n",
        "\n",
        "x_train_norm = x_norm_train.norm(x_train)\n",
        "x_test_norm = x_norm_val.norm(x_test)\n",
        "x_val_norm = x_norm_val.norm(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzFfUEzR6PZ6"
      },
      "outputs": [],
      "source": [
        "# Load datasets into Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))\n",
        "val_data = tf.data.Dataset.from_tensor_slices((x_val_norm, y_val))\n",
        "\n",
        "# Randomize and separate into batches\n",
        "batch_size = 400 # Train 400 time segments at a time\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size=x_train_norm.shape[0], seed=42).batch(batch_size=batch_size)\n",
        "test_data = test_data.shuffle(buffer_size=x_test_norm.shape[0], seed=42).batch(batch_size=batch_size)\n",
        "val_data = val_data.shuffle(buffer_size=x_val_norm.shape[0], seed=42).batch(batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi22s-LA_55a"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pmve9xgZLZr"
      },
      "outputs": [],
      "source": [
        "log_reg_model = LogRegModel()\n",
        "\n",
        "loop = TrainingLoop()\n",
        "log_reg_model = loop.train(train_data=train_data, val_data=val_data, test_data=test_data, \n",
        "                           num_epochs=300, learn_rate=0.05, model=log_reg_model, output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p0R5Pl6nHbv"
      },
      "source": [
        "#### Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJtQhFZ0_8o5"
      },
      "outputs": [],
      "source": [
        "# Plot loss and accuracy\n",
        "loop.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9r4hV1EmuEg"
      },
      "outputs": [],
      "source": [
        "# Create heatmap\n",
        "y_pred_train = log_reg_model(x_train_norm, train=False)\n",
        "y_pred_test = log_reg_model(x_test_norm, train=False)\n",
        "\n",
        "y_class_train = predict(y_pred_train)\n",
        "y_class_test = predict(y_pred_test)\n",
        "\n",
        "con_mat_train = confusion(y_pred=y_class_train, y_true=y_train, class_labels=class_labels, title='Training')\n",
        "con_mat_test = confusion(y_pred=y_class_test, y_true=y_test, class_labels=class_labels, title='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZKGaZZFtQth"
      },
      "outputs": [],
      "source": [
        "TN = con_mat_test[0][0]\n",
        "TP = con_mat_test[1][1]\n",
        "FN = con_mat_test[1][0]\n",
        "FP = con_mat_test[0][1]\n",
        "\n",
        "recall = TP / (TP + FN) # How many seizures were predicted correctly\n",
        "precision = TP / (TP + FP) # How many predicted seizures were actually positive\n",
        "f_score = 2 * (recall * precision) / (recall + precision) # Evaluates whether recall and precision are balanced\n",
        "\n",
        "print(f\"Recall = {recall:.3}\\nPrecision = {precision:.3}\\nF-Score = {f_score:.3}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export_module = ExportModule(model=log_reg_model, normalizer=x_test_norm, class_pred=predict)\n",
        "save_path = \"output/models/export_module\"\n",
        "tf.saved_model.save(export_module, save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_model = tf.saved_model.load(save_path)\n",
        "loaded_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ldIjCgMFCe5Y",
        "Q48PBfkwCxQC",
        "8Tp6mp8dDJLV",
        "MACto5tnqclq",
        "Oi22s-LA_55a"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.15 ('ml_ds')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d080c9af64691e792f3dcccc8e84323848f93a215ae2ff4265cf51c199f116ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
